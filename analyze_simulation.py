"""
Simulation Analysis Tool

This script helps visualize and analyze the realistic node simulation data.
It can be used to verify that the simulation is working correctly and to
understand the patterns generated by the nodes.
"""

import requests
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from typing import Dict, List
import numpy as np

# Configuration
PROMETHEUS_URL = "http://localhost:9090"
NODES = [f"node-{i}" for i in range(1, 16)]
COLLECTION_DURATION_SECONDS = 300  # 5 minutes


def fetch_prometheus_metric(query: str) -> Dict:
    """Fetch metric from Prometheus instant query."""
    try:
        response = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={"query": query},
            timeout=10
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"Error fetching metric: {e}")
        return {}


def fetch_range_metric(query: str, duration_seconds: int) -> Dict:
    """Fetch metric range from Prometheus."""
    try:
        response = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query_range",
            params={
                "query": query,
                "start": time.time() - duration_seconds,
                "end": time.time(),
                "step": "15s"  # 15 second resolution
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"Error fetching range metric: {e}")
        return {}


def analyze_node_characteristics():
    """
    Fetch and display the unique characteristics of each node.
    This shows how each node has different baseline parameters.
    """
    print("\n" + "="*80)
    print("NODE BASELINE CHARACTERISTICS ANALYSIS")
    print("="*80)
    
    data = []
    
    for node in NODES:
        # Fetch average latency
        latency_query = f'avg_over_time(request_latency_seconds{{node_id="{node}"}}[5m])'
        latency_data = fetch_prometheus_metric(latency_query)
        
        # Fetch error rate
        error_query = f'rate(http_requests_total{{node_id="{node}", status="500"}}[5m])'
        error_data = fetch_prometheus_metric(error_query)
        
        # Fetch CPU usage
        cpu_query = f'node_cpu_usage_percent{{node_id="{node}"}}'
        cpu_data = fetch_prometheus_metric(cpu_query)
        
        # Fetch memory usage
        mem_query = f'node_memory_mb{{node_id="{node}"}}'
        mem_data = fetch_prometheus_metric(mem_query)
        
        # Extract values
        avg_latency = "N/A"
        if latency_data.get("data", {}).get("result"):
            avg_latency = float(latency_data["data"]["result"][0]["value"][1])
            avg_latency = f"{avg_latency*1000:.2f}ms"
        
        error_rate = "N/A"
        if error_data.get("data", {}).get("result"):
            error_rate = float(error_data["data"]["result"][0]["value"][1])
            error_rate = f"{error_rate*100:.2f}%"
        
        cpu = "N/A"
        if cpu_data.get("data", {}).get("result"):
            cpu = float(cpu_data["data"]["result"][0]["value"][1])
            cpu = f"{cpu:.1f}%"
        
        memory = "N/A"
        if mem_data.get("data", {}).get("result"):
            memory = float(mem_data["data"]["result"][0]["value"][1])
            memory = f"{memory:.1f}MB"
        
        data.append({
            "Node": node,
            "Avg Latency": avg_latency,
            "Error Rate": error_rate,
            "CPU Usage": cpu,
            "Memory": memory
        })
    
    df = pd.DataFrame(data)
    print(df.to_string(index=False))
    print()
    return df


def plot_latency_distribution():
    """
    Plot latency distribution across all nodes to show variance and noise.
    """
    print("Generating latency distribution plot...")
    
    fig, axes = plt.subplots(3, 5, figsize=(20, 12))
    fig.suptitle('Latency Distribution by Node (Showing Realistic Noise)', fontsize=16)
    axes = axes.flatten()
    
    for idx, node in enumerate(NODES):
        query = f'request_latency_seconds{{node_id="{node}"}}'
        data = fetch_range_metric(query, COLLECTION_DURATION_SECONDS)
        
        if data.get("data", {}).get("result"):
            values = [float(v[1]) * 1000 for v in data["data"]["result"][0]["values"]]
            
            axes[idx].hist(values, bins=30, alpha=0.7, color='blue', edgecolor='black')
            axes[idx].set_title(f'{node}', fontsize=10)
            axes[idx].set_xlabel('Latency (ms)', fontsize=8)
            axes[idx].set_ylabel('Frequency', fontsize=8)
            
            # Add statistics
            mean = np.mean(values)
            std = np.std(values)
            axes[idx].axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.1f}ms')
            axes[idx].axvline(mean + std, color='orange', linestyle=':', alpha=0.5)
            axes[idx].axvline(mean - std, color='orange', linestyle=':', alpha=0.5)
            axes[idx].legend(fontsize=6)
    
    plt.tight_layout()
    plt.savefig('latency_distribution.png', dpi=300)
    print("Saved: latency_distribution.png")
    plt.close()


def plot_load_patterns():
    """
    Plot CPU usage over time to show time-based load variations.
    """
    print("Generating load pattern plot...")
    
    fig, ax = plt.subplots(figsize=(16, 8))
    
    for node in NODES[:5]:  # Sample first 5 nodes
        query = f'node_cpu_usage_percent{{node_id="{node}"}}'
        data = fetch_range_metric(query, COLLECTION_DURATION_SECONDS)
        
        if data.get("data", {}).get("result"):
            result = data["data"]["result"][0]
            timestamps = [datetime.fromtimestamp(v[0]) for v in result["values"]]
            values = [float(v[1]) for v in result["values"]]
            
            ax.plot(timestamps, values, label=node, alpha=0.7, linewidth=2)
    
    ax.set_xlabel('Time', fontsize=12)
    ax.set_ylabel('CPU Usage (%)', fontsize=12)
    ax.set_title('Time-Based Load Variation (Showing Sinusoidal Patterns + Spikes)', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('load_patterns.png', dpi=300)
    print("Saved: load_patterns.png")
    plt.close()


def analyze_byzantine_behavior():
    """
    Analyze probabilistic Byzantine fault patterns.
    """
    print("\n" + "="*80)
    print("BYZANTINE BEHAVIOR ANALYSIS (Probabilistic Faults)")
    print("="*80)
    
    data = []
    
    for node in NODES:
        # Total requests
        total_query = f'sum(increase(http_requests_total{{node_id="{node}"}}[5m]))'
        total_data = fetch_prometheus_metric(total_query)
        
        # 500 errors
        error_query = f'sum(increase(http_requests_total{{node_id="{node}", status="500"}}[5m]))'
        error_data = fetch_prometheus_metric(error_query)
        
        total = 0
        if total_data.get("data", {}).get("result"):
            total = int(float(total_data["data"]["result"][0]["value"][1]))
        
        errors = 0
        if error_data.get("data", {}).get("result"):
            errors = int(float(error_data["data"]["result"][0]["value"][1]))
        
        error_rate = (errors / total * 100) if total > 0 else 0
        
        # Classify behavior
        classification = "Benign"
        if error_rate > 20:
            classification = "Byzantine (500-error)"
        elif error_rate > 5:
            classification = "Suspicious"
        
        data.append({
            "Node": node,
            "Total Requests": total,
            "500 Errors": errors,
            "Error Rate": f"{error_rate:.1f}%",
            "Classification": classification
        })
    
    df = pd.DataFrame(data)
    print(df.to_string(index=False))
    print()
    
    # Plot
    fig, ax = plt.subplots(figsize=(14, 6))
    nodes = df["Node"]
    error_rates = [float(r.strip('%')) for r in df["Error Rate"]]
    colors = ['red' if c.startswith("Byzantine") else 'orange' if c == "Suspicious" else 'green' 
              for c in df["Classification"]]
    
    ax.bar(nodes, error_rates, color=colors, alpha=0.7, edgecolor='black')
    ax.set_xlabel('Node', fontsize=12)
    ax.set_ylabel('Error Rate (%)', fontsize=12)
    ax.set_title('Probabilistic Byzantine Fault Rates', fontsize=14)
    ax.axhline(20, color='red', linestyle='--', label='Byzantine Threshold (20%)', alpha=0.5)
    ax.axhline(5, color='orange', linestyle='--', label='Suspicious Threshold (5%)', alpha=0.5)
    ax.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('byzantine_analysis.png', dpi=300)
    print("Saved: byzantine_analysis.png")
    plt.close()


def analyze_network_noise():
    """
    Analyze network jitter and packet loss effects.
    """
    print("\n" + "="*80)
    print("NETWORK NOISE ANALYSIS (Jitter & Packet Loss)")
    print("="*80)
    
    for node in NODES[:3]:  # Sample first 3 nodes
        query = f'request_latency_seconds{{node_id="{node}"}}'
        data = fetch_range_metric(query, 60)  # Last minute
        
        if data.get("data", {}).get("result"):
            values = [float(v[1]) * 1000 for v in data["data"]["result"][0]["values"]]
            
            # Calculate statistics
            mean = np.mean(values)
            std = np.std(values)
            max_val = np.max(values)
            min_val = np.min(values)
            
            # Detect potential packet loss events (outliers)
            threshold = mean + 3 * std
            outliers = [v for v in values if v > threshold]
            packet_loss_events = len(outliers)
            
            print(f"\n{node}:")
            print(f"  Mean Latency: {mean:.2f}ms")
            print(f"  Std Dev (Jitter): {std:.2f}ms")
            print(f"  Range: {min_val:.2f}ms - {max_val:.2f}ms")
            print(f"  Potential Packet Loss Events: {packet_loss_events}")


def main():
    """
    Main analysis function.
    Run this after the system has been running for at least 5 minutes.
    """
    print("\n" + "="*80)
    print("REALISTIC NODE SIMULATION ANALYSIS")
    print("="*80)
    print(f"Prometheus URL: {PROMETHEUS_URL}")
    print(f"Collection Duration: {COLLECTION_DURATION_SECONDS}s")
    print(f"Analyzing {len(NODES)} nodes...")
    print()
    
    # Check Prometheus connection
    try:
        response = requests.get(f"{PROMETHEUS_URL}/api/v1/status/config", timeout=5)
        response.raise_for_status()
        print("✓ Prometheus connection successful")
    except Exception as e:
        print(f"✗ Cannot connect to Prometheus: {e}")
        print("Make sure the system is running: docker-compose up -d")
        return
    
    # Run analyses
    print("\n[1/5] Analyzing node characteristics...")
    analyze_node_characteristics()
    
    print("\n[2/5] Analyzing network noise...")
    analyze_network_noise()
    
    print("\n[3/5] Analyzing Byzantine behavior...")
    analyze_byzantine_behavior()
    
    print("\n[4/5] Generating latency distribution plots...")
    plot_latency_distribution()
    
    print("\n[5/5] Generating load pattern plots...")
    plot_load_patterns()
    
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE")
    print("="*80)
    print("\nGenerated files:")
    print("  - latency_distribution.png")
    print("  - load_patterns.png")
    print("  - byzantine_analysis.png")
    print()


if __name__ == "__main__":
    main()
